{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#ライブラリの自動リロード\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.GGUFBot import GGUFBot\n",
    "from src.HFDataset import HFDataset\n",
    "from src.SimpleQuestionGenerator import SimpleQuestionGenerator\n",
    "from src.AnswerGenerator import AnswerGenerator\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from src.GGUFEvaluator import prepare_prompt,parse_output,GGUFEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: additional 3 GGUFs metadata loaded.\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 563 tensors from /home/hatakeyama/python/ChatServer/model/Mixtral-8x22B-Instruct-v0.1.Q5_K_M-00001-of-00004.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models--mistralai--Mixtral-8x22B-Inst...\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 56\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 65536\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 6144\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 48\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  11:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                           llama.vocab_size u32              = 32768\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32768]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32768]   = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  26:                                   split.no u16              = 0\n",
      "llama_model_loader: - kv  27:                                split.count u16              = 4\n",
      "llama_model_loader: - kv  28:                        split.tensors.count i32              = 563\n",
      "llama_model_loader: - type  f32:  113 tensors\n",
      "llama_model_loader: - type  f16:   56 tensors\n",
      "llama_model_loader: - type q8_0:  112 tensors\n",
      "llama_model_loader: - type q5_K:  253 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 1027/32768 vs 259/32768 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32768\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 65536\n",
      "llm_load_print_meta: n_embd           = 6144\n",
      "llm_load_print_meta: n_head           = 48\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 56\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 6\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 65536\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8x22B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 140.63 B\n",
      "llm_load_print_meta: model size       = 93.11 GiB (5.69 BPW) \n",
      "llm_load_print_meta: general.name     = models--mistralai--Mixtral-8x22B-Instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 781 '<0x0A>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n",
      "ggml_cuda_init: found 8 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 1: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 2: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 3: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 4: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 5: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 6: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 7: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    2.51 MiB\n",
      "llm_load_tensors: offloading 56 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 57/57 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   132.00 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 12238.73 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size = 11728.73 MiB\n",
      "llm_load_tensors:      CUDA2 buffer size = 11728.73 MiB\n",
      "llm_load_tensors:      CUDA3 buffer size = 11830.73 MiB\n",
      "llm_load_tensors:      CUDA4 buffer size = 11728.73 MiB\n",
      "llm_load_tensors:      CUDA5 buffer size = 11728.73 MiB\n",
      "llm_load_tensors:      CUDA6 buffer size = 13579.12 MiB\n",
      "llm_load_tensors:      CUDA7 buffer size = 10647.87 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA2 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA3 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA4 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA5 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA6 KV buffer size =   125.00 MiB\n",
      "llama_kv_cache_init:      CUDA7 KV buffer size =    93.75 MiB\n",
      "llama_new_context_with_model: KV self size  =  875.00 MiB, K (f16):  437.50 MiB, V (f16):  437.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA2 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA3 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA4 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA5 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA6 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA7 compute buffer size =   502.27 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    43.27 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2638\n",
      "llama_new_context_with_model: graph splits = 9\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'split.no': '0', 'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ ' [INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + ' ' + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'split.count': '4', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '32768', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '6144', 'llama.feed_forward_length': '16384', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '48', 'llama.block_count': '56', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'split.tensors.count': '563', 'llama.context_length': '65536', 'general.name': 'models--mistralai--Mixtral-8x22B-Instruct-v0.1', 'llama.expert_used_count': '2', 'general.file_type': '17'}\n",
      "Using gguf chat template: {{bos_token}}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ ' [INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + ' ' + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path=\"/home/hatakeyama/python/ChatServer/model/Mixtral-8x22B-Instruct-v0.1.Q5_K_M-00001-of-00004.gguf\"\n",
    "max_new_tokens=4000\n",
    "n_layers=400\n",
    "bot = GGUFBot(model_path, max_new_tokens=max_new_tokens, n_ctx=max_new_tokens, n_gpu_layers=n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2985.50 ms\n",
      "llama_print_timings:      sample time =     114.43 ms /   534 runs   (    0.21 ms per token,  4666.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1300.28 ms /   135 tokens (    9.63 ms per token,   103.82 tokens per second)\n",
      "llama_print_timings:        eval time =  100137.71 ms /   533 runs   (  187.88 ms per token,     5.32 tokens per second)\n",
      "llama_print_timings:       total time =  102610.96 ms /   668 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' この問題を解くためには、比例の原理を用います。\\n\\n1. 金が水の19倍の重さであることから、金の量が1つの単位（たとえば、グラム）であれば、水は1/19単位に相当します。\\n2. 銅が水の9倍の重さであることから、銅の量が1つの単位（たとえば、グラム）であれば、水は1/9単位に相当します。\\n3. 合金を得るには、水の15倍の重さにならなくてはなりませんが、金と銅の比例割合は変わりません。それぞれの要素の量を決めるために、比例の原理を用いることができます。\\n4. 次のような方程式を立てます：$x \\\\cdot 19 + y \\\\cdot 9 = 15$ (ここで、$x$は金の量、$y$は銅の量を表します)。\\n5. この方程式を解くために、$y = \\\\frac{15 - 19x}{9}$ と変形します。\\n6. $x$と$y$の両者が整数である必要があることから、$x=3$と$y=2$を選びます（他にも解は存在しますが、この例では$x=3$, $y=2$が最も簡単な解です）。\\n7. よって、金と銅の比例割合は3:2です。つまり、合金を得るには、金を水の3倍（19 \\\\* 3 = 57倍）と銅を水の2倍（9 \\\\* 2 = 18倍）の重さ分混合する必要があります。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=\"\"\"次の文章を､日本の文化に即した平易な文章に書き直してください\n",
    "#文章\n",
    "次の記事を読んで、最も適切な回答を選択してください。記事: 昔ながらの遊び - 有料の子供向け!ボールを飲みに来てください！あるいは60,000個！アメリカ全土のショッピングモールや近所に、おもちゃやゲーム、ガイド付きの楽しみ、家計に負担をかけないトレーニングなどを特徴とする、従量制の屋内遊び場を提供する新しいタイプのビジネスフランチャイズが登場している。公共の遊び場がますます磨耗して汚くなる中、営利センターでは、清潔で安全なガイド付きアクティビティや、子供の体力を伸ばすためのさまざまなやりがいのあるエクササイズを、通常は 1 時間あたり約 5 ドルの料金で提供しています。カンザスシティに本拠を置くチェーン店、ニューヨーク州ヨンカーズにある設立2か月のディスカバリー・ゾーンのオーナー、ディック・グッゲンハイマー氏は、「遊び場は汚いし、案内もされていない」と語る。安全です」。共働き世帯のニーズを満たすために、この新しいフランチャイズは、従来の公共の遊び場が暗くなって使用できなくなってからもずっと夜も営業しています。ただし、これらの新しい遊び場は保育所を目的としたものではありません。親は子供を送り出すのではなく、家にいて一緒に遊ぶことが期待されています。しかし、ハイテクベビーシッターサービスを提供しているところもあります。ディスカバリー ゾーンの一部では、親が子供を特別なガイド付きプログラムに登録した後、子供を残して数時間こっそりと映画やディナーを楽しむことができます。何か問題があれば、お父さんとお母さんが呼ばれます。しかし、何よりも楽しいのは、共働き家庭で通勤に 2 時間かかる以前の時代に親がやっていたようなこと、子供たちと遊ぶことができるようになることです。少なくとも、時間当たりの料金を考えても、これは時代遅れです。質問: この記事は主に何について話していますか? - お子様は親の世話なしで遊ぶことができます。 - ディスカバリーゾーンの急速な発展。 - 新しいタイプのキッズビジネスフランチャイズ。 - 屋外遊び場の欠点。 この質問に対する答えは次のとおりです\n",
    "#書き直した文章\n",
    "\"\"\"\n",
    "txt=\"\"\"step by stepで､日本語で回答してください\n",
    "タスクは、与えられた数学の問題に対する正しい答えを生成することです。 Q: 問題: 金は水の 19 倍、銅は水の 9 倍の重さです。水の15倍の重さの合金を得るには、これらをどのような割合で混合すればよいでしょうか? 答え: \"\"\"\n",
    "r=bot.ask(txt)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 30.0/30.0 [00:00<00:00, 65.2kB/s]\n",
      "Downloading data:  11%|█▏        | 331M/2.93G [01:59<15:32, 2.78MB/s] "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds=load_dataset(\"atsushi3110/cross-lingual-openorcha-830k-en-ja\",split=\"train\")\n",
    "df=pd.DataFrame(ds)\n",
    "df[\"database\"]=\"atsushi3110/cross-lingual-openorcha-830k-en-ja_\"+df[\"id/en\"]\n",
    "df[\"question\"]=df[\"question/ja\"]\n",
    "df=df.drop(columns=[\"response/en\",\"system_prompt/en\"],inplace=True)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "ds_name=\"hatakeyama-llm-team/WikiBookJa\"\n",
    "ds=load_dataset(ds_name,split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_list=[\n",
    "\"回答文は丁寧であること\"\n",
    "\"回答文は簡潔であること\"\n",
    "\"回答文はstep by stepで作文してください\",\n",
    "]\n",
    "\n",
    "def gen_prompt(inst,text):\n",
    "    prompt_template=f\"\"\"次の文章をもとに､日本語の質問文と日本語の回答文をそれぞれ一つ生成しなさい\n",
    "    #制約\n",
    "    - {inst}\n",
    "    #文章\n",
    "    {text}\n",
    "    #質問文と回答文\n",
    "    \"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1630.62 ms\n",
      "llama_print_timings:      sample time =      55.26 ms /   257 runs   (    0.22 ms per token,  4650.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5278.59 ms /  1227 tokens (    4.30 ms per token,   232.45 tokens per second)\n",
      "llama_print_timings:        eval time =   27329.40 ms /   256 runs   (  106.76 ms per token,     9.37 tokens per second)\n",
      "llama_print_timings:       total time =   33111.03 ms /  1483 tokens\n"
     ]
    }
   ],
   "source": [
    "record=ds[random.randint(0,len(ds))]\n",
    "prompt=gen_prompt(random.choice(inst_list)[:1000],record[\"text\"])\n",
    "r=bot.ask(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('日本の民法では、利息を元本に組入れることができる条件となっていますか？',\n",
       " 'はい、日本の民法においては、利息を元本に組入れることができる条件が定められています。具体的には、債権者が催告をしても、債務者がその利息を支払わなかった場合に、債権者は当該利息債権を元本に組み入れることができます。ただし、1年以上の不払いと催告後の不払いが条件となり、単利計算が原則であることから、手続きを定めた規準としての存在意義は薄いものと考えられます。')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,a=r.split(\"回答文\")\n",
    "q=q.replace(\"質問文\",\"\").strip()\n",
    "a=a.strip()\n",
    "if a[0]==\"：\":\n",
    "    a=a[1:]\n",
    "if q[0]==\"：\":\n",
    "    q=q[1:]\n",
    "if a[0]==\":\":\n",
    "    a=a[1:]\n",
    "if q[0]==\":\":\n",
    "    q=q[1:]\n",
    "\n",
    "a=a.strip()\n",
    "q=q.strip()\n",
    "q,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 質問: この場所にはどんな種類の情報が集まっていますか？\\n回答: この書庫には、音楽に関する様々な種類の情報が収録されています。これらの情報は、文書や資料、教科書等で提供されています。\\n\\n質問: 収録内容を具体的に教えてください。\\n回答: 申し訳ありませんが、この文章からは具体的な収録内容について情報を提供できません。書庫内の音楽に関する各種情報を確認するためには、書庫に直接アクセスしてください。'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
